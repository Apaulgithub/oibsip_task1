{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "KSlN3yHqYklG",
        "EM7whBJCYoAo",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "OH-pJp9IphqM",
        "yLjJCtPM0KBk",
        "89xtkJwZ18nB",
        "rMDnDkt2B6du",
        "BhH2vgX9EjGr",
        "VfCC591jGiD4",
        "mWppNeqWwA-M",
        "pTi0g55fwA-W",
        "iv0m-lDQwA-W",
        "VzKNBLqiwA-Y",
        "1GL-36pywA-Y",
        "nWFgbYYVwA-Z",
        "841njF38wA-Z",
        "Bgen1cFIwA-a",
        "JPT_4dYWwA-a",
        "LRvyaI9BwA-b",
        "c-26ctgmwA-c",
        "HO3sJ66GwA-c",
        "lHIyvCjC_4_G",
        "zlBiexnN_4_G",
        "ffEiQ19I_4_H",
        "Bbpe4TaP_4_H",
        "2CnsMkMiM-8g",
        "axoYmkZZM-8g",
        "YcglZXVVM-8h",
        "pVXGvXVb_6FF",
        "1R6e5Trb_6FF",
        "zhaPpYXo_6FF",
        "ZYFMyHqX_6FG",
        "t4hU8F0I_6FG",
        "1NNpISRdaSng",
        "6IvVWbfDaSnh",
        "0bSwMgKcaSnh",
        "IXHnAaJfaSni",
        "BSoGk_2RaSnj",
        "UisOqWiDb6SZ",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "gCX9965dhzqZ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apaulgithub/oibsip_task1/blob/main/Iris_Flower_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Iris Flower Classification\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Industry**    - Oasis Infobyte\n",
        "##### **Contribution**    - Individual\n",
        "##### **Member Name -** Arindam Paul\n",
        "##### **Task -** 1\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Description:**\n",
        "\n",
        "The Iris Flower Classification project focuses on developing a machine learning model to classify iris flowers into their respective species based on specific measurements. Iris flowers are classified into three species: setosa, versicolor, and virginica, each of which exhibits distinct characteristics in terms of measurements.\n",
        "\n",
        "**Objective:**\n",
        "\n",
        "The primary goal of this project is to leverage machine learning techniques to build a classification model that can accurately identify the species of iris flowers based on their measurements. The model aims to automate the classification process, offering a practical solution for identifying iris species.\n",
        "\n",
        "**Key Project Details:**\n",
        "\n",
        "- Iris flowers have three species: setosa, versicolor, and virginica.\n",
        "- These species can be distinguished based on measurements such as sepal length, sepal width, petal length, and petal width.\n",
        "- The project involves training a machine learning model on a dataset that contains iris flower measurements associated with their respective species.\n",
        "- The trained model will classify iris flowers into one of the three species based on their measurements."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GitHub Link -** https://github.com/Apaulgithub/oibsip_task1/tree/main"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The iris flower, scientifically known as Iris, is a distinctive genus of flowering plants. Within this genus, there are three primary species: Iris setosa, Iris versicolor, and Iris virginica. These species exhibit variations in their physical characteristics, particularly in the measurements of their sepal length, sepal width, petal length, and petal width.\n",
        "\n",
        "**Objective:**\n",
        "\n",
        "The objective of this project is to develop a machine learning model capable of learning from the measurements of iris flowers and accurately classifying them into their respective species. The model's primary goal is to automate the classification process based on the distinct characteristics of each iris species.\n",
        "\n",
        "**Project Details:**\n",
        "\n",
        "- **Iris Species:** The dataset consists of iris flowers, specifically from the species setosa, versicolor, and virginica.\n",
        "- **Key Measurements:** The essential characteristics used for classification include sepal length, sepal width, petal length, and petal width.\n",
        "- **Machine Learning Model:** The project involves the creation and training of a machine learning model to accurately classify iris flowers based on their measurements.\n",
        "\n",
        "This project's significance lies in its potential to streamline and automate the classification of iris species, which can have broader applications in botany, horticulture, and environmental monitoring."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know The Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Importing Numpy & Pandas for data processing & data wrangling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Importing  tools for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Import evaluation metric libraries\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Library used for data preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Import model selection libraries\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, RepeatedStratifiedKFold\n",
        "\n",
        "# Library used for ML Model implementation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import xgboost as xgb\n",
        "\n",
        "# Library used for ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/Apaulgithub/oibsip_task1/main/Iris.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# View top 5 rows of the dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "# Checking number of rows and columns of the dataset using shape\n",
        "print(\"Number of rows are: \",df.shape[0])\n",
        "print(\"Number of columns are: \",df.shape[1])"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "# Checking information about the dataset using info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "dup = df.duplicated().sum()\n",
        "print(f'number of duplicated rows are {dup}')"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did i know about the dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The Iris dataset consists of length and width mesurements of sepal and petal for different species in centimeter.\n",
        "* There are 150 rows and 6 columns provided in the data.\n",
        "* No duplicate values exist.\n",
        "* No Null values exist."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding The Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe (all columns included)\n",
        "df.describe(include= 'all').round(2)"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in\",i,\"is\",df[i].nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***3. Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We don't need the 1st column so let's drop that\n",
        "data=df.iloc[:,1:]"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New updated dataset\n",
        "data.head()"
      ],
      "metadata": {
        "id": "LLjNXM30tBZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have i done?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only drop the first column of the dataset."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 : Distribution of Numerical Variables"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 Histogram visualization code for distribution of numerical variables\n",
        "# Create a figure with subplots\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.suptitle('Distribution of Iris Flower Measurements', fontsize=14)\n",
        "\n",
        "# Create a 2x2 grid of subplots\n",
        "plt.subplot(2, 2, 1)  # Subplot 1 (Top-Left)\n",
        "plt.hist(data['SepalLengthCm'])\n",
        "plt.title('Sepal Length Distribution')\n",
        "\n",
        "plt.subplot(2, 2, 2)  # Subplot 2 (Top-Right)\n",
        "plt.hist(data['SepalWidthCm'])\n",
        "plt.title('Sepal Width Distribution')\n",
        "\n",
        "plt.subplot(2, 2, 3)  # Subplot 3 (Bottom-Left)\n",
        "plt.hist(data['PetalLengthCm'])\n",
        "plt.title('Petal Length Distribution')\n",
        "\n",
        "plt.subplot(2, 2, 4)  # Subplot 4 (Bottom-Right)\n",
        "plt.hist(data['PetalWidthCm'])\n",
        "plt.title('Petal Width Distribution')\n",
        "\n",
        "# Display the subplots\n",
        "plt.tight_layout()  # Helps in adjusting the layout\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 : Sepal Length vs Sepal Width"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define colors for each species and the corresponding species labels.\n",
        "colors = ['red', 'yellow', 'green']\n",
        "species = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']"
      ],
      "metadata": {
        "id": "pmeReNPIwpsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 Scatter plot visualization code for Sepal Length vs Sepal Width.\n",
        "# Create a scatter plot for Sepal Length vs Sepal Width for each species.\n",
        "for i in range(3):\n",
        "    # Select data for the current species.\n",
        "    x = data[data['Species'] == species[i]]\n",
        "\n",
        "    # Create a scatter plot with the specified color and label for the current species.\n",
        "    plt.scatter(x['SepalLengthCm'], x['SepalWidthCm'], c=colors[i], label=species[i])\n",
        "\n",
        "# Add labels to the x and y axes.\n",
        "plt.xlabel('Sepal Length')\n",
        "plt.ylabel('Sepal Width')\n",
        "\n",
        "# Add a legend to identify species based on colors.\n",
        "plt.legend()\n",
        "\n",
        "# Display the scatter plot.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 : Petal Length vs Petal Width"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 Scatter plot visualization code for Petal Length vs Petal Width.\n",
        "# Create a scatter plot for Petal Length vs Petal Width for each species.\n",
        "for i in range(3):\n",
        "    # Select data for the current species.\n",
        "    x = data[data['Species'] == species[i]]\n",
        "\n",
        "    # Create a scatter plot with the specified color and label for the current species.\n",
        "    plt.scatter(x['PetalLengthCm'], x['PetalWidthCm'], c=colors[i], label=species[i])\n",
        "\n",
        "# Add labels to the x and y axes.\n",
        "plt.xlabel('Petal Length')\n",
        "plt.ylabel('Petal Width')\n",
        "\n",
        "# Add a legend to identify species based on colors.\n",
        "plt.legend()\n",
        "\n",
        "# Display the scatter plot.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 : Sepal Length vs Petal Length"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 Scatter plot visualization code for Sepal Length vs Petal Length.\n",
        "# Create a scatter plot for Sepal Length vs Petal Length for each species.\n",
        "for i in range(3):\n",
        "    # Select data for the current species.\n",
        "    x = data[data['Species'] == species[i]]\n",
        "\n",
        "    # Create a scatter plot with the specified color and label for the current species.\n",
        "    plt.scatter(x['SepalLengthCm'], x['PetalLengthCm'], c=colors[i], label=species[i])\n",
        "\n",
        "# Add labels to the x and y axes.\n",
        "plt.xlabel('Sepal Length')\n",
        "plt.ylabel('Petal Length')\n",
        "\n",
        "# Add a legend to identify species based on colors.\n",
        "plt.legend()\n",
        "\n",
        "# Display the scatter plot.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 : Sepal Width vs Petal Width"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 Scatter plot visualization code for Sepal Width vs Petal Width.\n",
        "# Create a scatter plot for Sepal Width vs Petal Width for each species.\n",
        "for i in range(3):\n",
        "    # Select data for the current species.\n",
        "    x = data[data['Species'] == species[i]]\n",
        "\n",
        "    # Create a scatter plot with the specified color and label for the current species.\n",
        "    plt.scatter(x['SepalWidthCm'], x['PetalWidthCm'], c=colors[i], label=species[i])\n",
        "\n",
        "# Add labels to the x and y axes.\n",
        "plt.xlabel('Sepal Width')\n",
        "plt.ylabel('Petal Width')\n",
        "\n",
        "# Add a legend to identify species based on colors.\n",
        "plt.legend()\n",
        "\n",
        "# Display the scatter plot.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 : Correlation Heatmap"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap Visualization Code\n",
        "corr_matrix = data.corr()\n",
        "\n",
        "# Plot Heatmap\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='Reds_r')\n",
        "\n",
        "# Setting Labels\n",
        "plt.title('Correlation Matrix heatmap')\n",
        "\n",
        "# Display Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the categorical columns\n",
        "# Create a LabelEncoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Encode the 'Species' column to convert the species names to numerical labels\n",
        "data['Species'] = le.fit_transform(data['Species'])\n",
        "\n",
        "# Check the unique values in the 'Species' column after encoding\n",
        "unique_species = data['Species'].unique()\n",
        "\n",
        "# Display the unique encoded values\n",
        "print(\"Encoded Species Values:\")\n",
        "print(unique_species) # 'Iris-setosa' == 0, 'Iris-versicolor' == 1, 'Iris-virginica' == 2"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the X and y\n",
        "x=data.drop(columns=['Species'], axis=1)\n",
        "y=data['Species']"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data to train and test\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y, test_size=0.3)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the train distribution of dependent variable\n",
        "y_train.value_counts()"
      ],
      "metadata": {
        "id": "-fRxg3Xr9g_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, x_train, x_test, y_train, y_test):\n",
        "    '''The function will take model, x train, x test, y train, y test\n",
        "    and then it will fit the model, then make predictions on the trained model,\n",
        "    it will then print roc-auc score of train and test, then plot the roc, auc curve,\n",
        "    print confusion matrix for train and test, then print classification report for train and test,\n",
        "    then plot the feature importances if the model has feature importances,\n",
        "    and finally it will return the following scores as a list:\n",
        "    recall_train, recall_test, acc_train, acc_test, F1_train, F1_test\n",
        "    '''\n",
        "\n",
        "    # Fit the model to the training data.\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    # make predictions on the test data\n",
        "    y_pred_train = model.predict(x_train)\n",
        "    y_pred_test = model.predict(x_test)\n",
        "\n",
        "    # calculate confusion matrix\n",
        "    cm_train = confusion_matrix(y_train, y_pred_train)\n",
        "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(11,4))\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    sns.heatmap(cm_train, annot=True, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], cmap=\"Oranges\", fmt='.4g', ax=ax[0])\n",
        "    ax[0].set_xlabel(\"Predicted Label\")\n",
        "    ax[0].set_ylabel(\"True Label\")\n",
        "    ax[0].set_title(\"Train Confusion Matrix\")\n",
        "\n",
        "    sns.heatmap(cm_test, annot=True, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], cmap=\"Oranges\", fmt='.4g', ax=ax[1])\n",
        "    ax[1].set_xlabel(\"Predicted Label\")\n",
        "    ax[1].set_ylabel(\"True Label\")\n",
        "    ax[1].set_title(\"Test Confusion Matrix\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # calculate classification report\n",
        "    cr_train = classification_report(y_train, y_pred_train, output_dict=True)\n",
        "    cr_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
        "    print(\"\\nTrain Classification Report:\")\n",
        "    crt = pd.DataFrame(cr_train).T\n",
        "    print(crt.to_markdown())\n",
        "    # sns.heatmap(pd.DataFrame(cr_train).T.iloc[:, :-1], annot=True, cmap=\"Blues\")\n",
        "    print(\"\\nTest Classification Report:\")\n",
        "    crt2 = pd.DataFrame(cr_test).T\n",
        "    print(crt2.to_markdown())\n",
        "    # sns.heatmap(pd.DataFrame(cr_test).T.iloc[:, :-1], annot=True, cmap=\"Blues\")\n",
        "\n",
        "    precision_train = cr_train['weighted avg']['precision']\n",
        "    precision_test = cr_test['weighted avg']['precision']\n",
        "\n",
        "    recall_train = cr_train['weighted avg']['recall']\n",
        "    recall_test = cr_test['weighted avg']['recall']\n",
        "\n",
        "    acc_train = accuracy_score(y_true = y_train, y_pred = y_pred_train)\n",
        "    acc_test = accuracy_score(y_true = y_test, y_pred = y_pred_test)\n",
        "\n",
        "    F1_train = cr_train['weighted avg']['f1-score']\n",
        "    F1_test = cr_test['weighted avg']['f1-score']\n",
        "\n",
        "    model_score = [precision_train, precision_test, recall_train, recall_test, acc_train, acc_test, F1_train, F1_test ]\n",
        "    return model_score"
      ],
      "metadata": {
        "id": "PPTAGKKZkMyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a score dataframe\n",
        "score = pd.DataFrame(index = ['Precision Train', 'Precision Test','Recall Train','Recall Test','Accuracy Train', 'Accuracy Test', 'F1 macro Train', 'F1 macro Test'])"
      ],
      "metadata": {
        "id": "SisvUGuimFkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 : Logistic regression"
      ],
      "metadata": {
        "id": "mWppNeqWwA-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "lr_model = LogisticRegression(fit_intercept=True, max_iter=10000)\n",
        "\n",
        "# Model is trained (fit) and predicted in the evaluate model"
      ],
      "metadata": {
        "id": "pjY8lKMfwA-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "pTi0g55fwA-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "lr_score = evaluate_model(lr_model, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "e7haAzk4wA-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Evaluation metric Score Chart\n",
        "score['Logistic regression'] = lr_score\n",
        "score"
      ],
      "metadata": {
        "id": "uBbjg5rNUzlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "iv0m-lDQwA-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {'C': [100,10,1,0.1,0.01,0.001,0.0001],\n",
        "              'penalty': ['l1', 'l2'],\n",
        "              'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
        "\n",
        "# Initializing the logistic regression model\n",
        "logreg = LogisticRegression(fit_intercept=True, max_iter=10000, random_state=0)\n",
        "\n",
        "# Repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=4, random_state=0)\n",
        "\n",
        "# Using GridSearchCV to tune the hyperparameters using cross-validation\n",
        "grid = GridSearchCV(logreg, param_grid, cv=rskf)\n",
        "grid.fit(x_train, y_train)\n",
        "\n",
        "# Select the best hyperparameters found by GridSearchCV\n",
        "best_params = grid.best_params_\n",
        "print(\"Best hyperparameters: \", best_params)"
      ],
      "metadata": {
        "id": "jqkHZjt7wA-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate model with best parameters\n",
        "lr_model2 = LogisticRegression(C=best_params['C'],\n",
        "                                  penalty=best_params['penalty'],\n",
        "                                  solver=best_params['solver'],\n",
        "                                  max_iter=10000, random_state=0)"
      ],
      "metadata": {
        "id": "bfWwIMtCVUf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "lr_score2 = evaluate_model(lr_model2, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "b9VFl9UaVWMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['Logistic regression tuned'] = lr_score2"
      ],
      "metadata": {
        "id": "TsqnpQW4Vnxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have i used and why?"
      ],
      "metadata": {
        "id": "mwnxeO7jwA-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used is GridSearchCV. GridSearchCV is a method that performs an exhaustive search over a specified parameter grid to find the best hyperparameters for a model. It is a popular method for hyperparameter tuning because it is simple to implement and can be effective in finding good hyperparameters for a model.\n",
        "\n",
        "The choice of hyperparameter optimization technique depends on various factors such as the size of the parameter space, the computational resources available, and the time constraints. GridSearchCV can be a good choice when the parameter space is relatively small and computational resources are not a major concern."
      ],
      "metadata": {
        "id": "qJdahr6QwA-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have i seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "huCCA590wA-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Evaluation metric Score Chart\n",
        "score"
      ],
      "metadata": {
        "id": "haB7fCvRWIdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that hyperparameter tuning did not improve the performance of the Logistic Regression model on the test set. The precision, recall, accuracy and F1 scores on the test set are the same for both the untuned and tuned Logistic Regression models."
      ],
      "metadata": {
        "id": "SmvtmJuQwA-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 : Decision Tree"
      ],
      "metadata": {
        "id": "VzKNBLqiwA-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "dt_model = DecisionTreeClassifier(random_state=20)\n",
        "\n",
        "# Model is trained (fit) and predicted in the evaluate model"
      ],
      "metadata": {
        "id": "EOs5hr_k8M5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "1GL-36pywA-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "dt_score = evaluate_model(dt_model, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "fz_4QHj2wA-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Evaluation metric Score Chart\n",
        "score['Decision Tree'] = dt_score\n",
        "score"
      ],
      "metadata": {
        "id": "tlG_TPuZB9vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "nWFgbYYVwA-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Define the hyperparameter grid\n",
        "grid = {'max_depth' : [3,4,5,6,7,8],\n",
        "        'min_samples_split' : np.arange(2,8),\n",
        "        'min_samples_leaf' : np.arange(10,20)}\n",
        "\n",
        "# Initialize the model\n",
        "model = DecisionTreeClassifier()\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=0)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(model, grid, cv=rskf)\n",
        "\n",
        "# Fit the GridSearchCV to the training data\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best hyperparameters: \", best_params)"
      ],
      "metadata": {
        "id": "FPAxyd04wA-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a new model with the best hyperparameters\n",
        "dt_model2 = DecisionTreeClassifier(max_depth=best_params['max_depth'],\n",
        "                                 min_samples_leaf=best_params['min_samples_leaf'],\n",
        "                                 min_samples_split=best_params['min_samples_split'],\n",
        "                                 random_state=20)"
      ],
      "metadata": {
        "id": "lFPV0F66Doc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "dt2_score = evaluate_model(dt_model2, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "M4htWLUADvI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['Decision Tree tuned'] = dt2_score"
      ],
      "metadata": {
        "id": "MPa2P97oEFVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have i used and why?"
      ],
      "metadata": {
        "id": "841njF38wA-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used is GridSearchCV. GridSearchCV is a method that performs an exhaustive search over a specified parameter grid to find the best hyperparameters for a model. It is a popular method for hyperparameter tuning because it is simple to implement and can be effective in finding good hyperparameters for a model.\n",
        "\n",
        "The choice of hyperparameter optimization technique depends on various factors such as the size of the parameter space, the computational resources available, and the time constraints. GridSearchCV can be a good choice when the parameter space is relatively small and computational resources are not a major concern."
      ],
      "metadata": {
        "id": "3Z1OoVu4wA-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have i seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "AkBX0MHbwA-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Evaluation metric Score Chart\n",
        "score"
      ],
      "metadata": {
        "id": "VknJyN6FEdc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that hyperparameter tuning improved the performance of the Decision Tree model on the train set. But the precision, recall, accuracy and F1 scores on the test set are the same for both the untuned and tuned Decision Tree models.\n",
        "\n",
        "The tuned model is not overfitting like the untuned model."
      ],
      "metadata": {
        "id": "XzZDDTLWwA-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 : Random Forest"
      ],
      "metadata": {
        "id": "Bgen1cFIwA-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "rf_model = RandomForestClassifier(random_state=0)\n",
        "\n",
        "# Model is trained (fit) and predicted in the evaluate model"
      ],
      "metadata": {
        "id": "6ihFy_gjwA-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JPT_4dYWwA-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "rf_score = evaluate_model(rf_model, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "o5z7_5MxwA-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Evaluation metric Score Chart\n",
        "score['Random Forest'] = rf_score\n",
        "score"
      ],
      "metadata": {
        "id": "YRaqdO4jumXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "LRvyaI9BwA-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Define the hyperparameter grid\n",
        "grid = {'n_estimators': [10, 50, 100, 200],\n",
        "              'max_depth': [8, 9, 10, 11, 12,13, 14, 15],\n",
        "              'min_samples_split': [2, 3, 4, 5]}\n",
        "\n",
        "# Initialize the model\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "\n",
        "# Repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=0)\n",
        "\n",
        "# Initialize RandomSearchCV\n",
        "random_search = RandomizedSearchCV(rf, grid,cv=rskf, n_iter=10, n_jobs=-1)\n",
        "\n",
        "# Fit the RandomSearchCV to the training data\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "print(\"Best hyperparameters: \", best_params)"
      ],
      "metadata": {
        "id": "dPxstoI0wA-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model with best parameters\n",
        "rf_model2 = RandomForestClassifier(n_estimators = best_params['n_estimators'],\n",
        "                                 min_samples_leaf= best_params['min_samples_split'],\n",
        "                                 max_depth = best_params['max_depth'],\n",
        "                                 random_state=0)"
      ],
      "metadata": {
        "id": "_kQLZcUVwHwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "rf2_score = evaluate_model(rf_model2, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "Jl8uTrpRwPAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['Random Forest tuned'] = rf2_score"
      ],
      "metadata": {
        "id": "AmhUFwxdwbSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have i used and why?"
      ],
      "metadata": {
        "id": "c-26ctgmwA-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique i used is RandomizedSearchCV. RandomizedSearchCV is a method that performs a random search over a specified parameter grid to find the best hyperparameters for a model. It is a popular method for hyperparameter tuning because it can be more efficient than exhaustive search methods like GridSearchCV when the parameter space is large.\n",
        "\n",
        "The choice of hyperparameter optimization technique depends on various factors such as the size of the parameter space, the computational resources available, and the time constraints. RandomizedSearchCV can be a good choice when the parameter space is large and computational resources are limited."
      ],
      "metadata": {
        "id": "IpCC2mf0wA-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have i seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "HO3sJ66GwA-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Evaluation metric Score Chart\n",
        "score"
      ],
      "metadata": {
        "id": "os_VXzbzxdX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that hyperparameter tuning improved the performance of the Random Forest model on the train set. But the precision, recall, accuracy and F1 scores on the test set are decreased in the tuned Random Forest model compare to the untuned Random Forest model."
      ],
      "metadata": {
        "id": "ta9Xk3xfwA-c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHIyvCjC_4_G"
      },
      "source": [
        "### ML Model - 4 : SVM (Support Vector Machine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1TNdgXg_4_G"
      },
      "outputs": [],
      "source": [
        "# ML Model - 4 Implementation\n",
        "svm_model = SVC(kernel='linear', random_state=0, probability=True)\n",
        "\n",
        "# Model is trained (fit) and predicted in the evaluate model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlBiexnN_4_G"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3Ic0Y-m_4_G"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "svm_score = evaluate_model(svm_model, x_train, x_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Evaluation metric Score Chart\n",
        "score['SVM'] = svm_score\n",
        "score"
      ],
      "metadata": {
        "id": "hah2ym_FI9YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffEiQ19I_4_H"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU8Saxiv_4_H"
      },
      "outputs": [],
      "source": [
        "# ML Model - 4 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {'C': np.arange(0.1, 10, 0.1),\n",
        "              'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "              'degree': np.arange(2, 6, 1)}\n",
        "\n",
        "# Initialize the model\n",
        "svm = SVC(random_state=0, probability=True)\n",
        "\n",
        "# Repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=0)\n",
        "\n",
        "# Initialize RandomizedSearchCV with kfold cross-validation\n",
        "random_search = RandomizedSearchCV(svm, param_grid, n_iter=10, cv=rskf, n_jobs=-1)\n",
        "\n",
        "# Fit the RandomizedSearchCV to the training data\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "print(\"Best hyperparameters: \", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model with best parameters\n",
        "svm_model2 = SVC(C = best_params['C'],\n",
        "           kernel = best_params['kernel'],\n",
        "           degree = best_params['degree'],\n",
        "           random_state=0, probability=True)"
      ],
      "metadata": {
        "id": "VytDm6E-JP-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "svm2_score = evaluate_model(svm_model2, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "DiLNfk1RJGXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['SVM tuned'] = svm2_score"
      ],
      "metadata": {
        "id": "5jhEgQWyJG3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bbpe4TaP_4_H"
      },
      "source": [
        "##### Which hyperparameter optimization technique have i used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NSc4Rh__4_H"
      },
      "source": [
        " Here Randomized search is used as a hyperparameter optimization technique.\n",
        " Randomized search is a popular technique because it can be more efficient than exhaustive search methods like grid search. Instead of trying all possible combinations of hyperparameters, randomized search samples a random subset of the hyperparameter space. This can save time and computational resources while still finding good hyperparameters for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifZ_nK19_4_H"
      },
      "source": [
        "##### Have i seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Evaluation metric Score Chart\n",
        "score"
      ],
      "metadata": {
        "id": "9Ei0SAaAKd6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvDgB1uo_4_H"
      },
      "source": [
        "It appears that hyperparameter tuning did not improve the performance of the SVM model on the test set. The precision, recall, accuracy and F1 scores on the test set are same for both the untuned and tuned SVM models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CnsMkMiM-8g"
      },
      "source": [
        "### ML Model - 5 : Xtreme Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENIrhtBEM-8g"
      },
      "outputs": [],
      "source": [
        "# ML Model - 5 Implementation\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "\n",
        "# Model is trained (fit) and predicted in the evaluate model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axoYmkZZM-8g"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guHO5uEVM-8g"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "xgb_score = evaluate_model(xgb_model, x_train, x_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Evaluation metric Score Chart\n",
        "score['XGB'] = xgb_score\n",
        "score"
      ],
      "metadata": {
        "id": "VhxqIfx0M-8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcglZXVVM-8h"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4epH9LEM-8h"
      },
      "outputs": [],
      "source": [
        "# ML Model - 5 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {'learning_rate': np.arange(0.01, 0.3, 0.01),\n",
        "              'max_depth': np.arange(3, 15, 1),\n",
        "              'n_estimators': np.arange(100, 200, 10)}\n",
        "\n",
        "# Initialize the model\n",
        "xgb2 = xgb.XGBClassifier(random_state=0)\n",
        "\n",
        "# Repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=0)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(xgb2, param_grid, n_iter=10, cv=rskf)\n",
        "\n",
        "# Fit the RandomizedSearchCV to the training data\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "print(\"Best hyperparameters: \", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model with best parameters\n",
        "xgb_model2 = xgb.XGBClassifier(learning_rate = best_params['learning_rate'],\n",
        "                                 max_depth = best_params['max_depth'],\n",
        "                               n_estimators = best_params['n_estimators'],\n",
        "                                 random_state=0)"
      ],
      "metadata": {
        "id": "EtNHi1dxM-8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "xgb2_score = evaluate_model(xgb_model2, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "0XR-RyTbM-8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['XGB tuned'] = xgb2_score"
      ],
      "metadata": {
        "id": "szQtnydCM-8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOuC2Vx2M-8i"
      },
      "source": [
        "##### Which hyperparameter optimization technique have i used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83SC9H7KM-8i"
      },
      "source": [
        "Here we have used Randomized search to tune the XGB model.\n",
        "\n",
        "Randomized search is a popular technique because it can be more efficient than exhaustive search methods like grid search. Instead of trying all possible combinations of hyperparameters, randomized search samples a random subset of the hyperparameter space. This can save time and computational resources while still finding good hyperparameters for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5YXAPbAM-8i"
      },
      "source": [
        "##### Have i seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Evaluation metric Score Chart\n",
        "score"
      ],
      "metadata": {
        "id": "aMKspbXUM-8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9AmUMHmM-8j"
      },
      "source": [
        "It appears that hyperparameter tuning did not improve the performance of the XGBoost model on the test set. The precision, recall, accuracy and F1 scores on the test set are same for both the untuned and tuned XGBoost models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVXGvXVb_6FF"
      },
      "source": [
        "### ML Model - 6 : Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ1kznd1_6FF"
      },
      "outputs": [],
      "source": [
        "# ML Model - 6 Implementation\n",
        "nb_model = GaussianNB()\n",
        "\n",
        "# Model is trained (fit) and predicted in the evaluate model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R6e5Trb_6FF"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7FEfxhp_6FF"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "nb_score = evaluate_model(nb_model, x_train, x_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Evaluation metric Score Chart\n",
        "score['Naive Bayes'] = nb_score\n",
        "score"
      ],
      "metadata": {
        "id": "Ck9Gav7DLYre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhaPpYXo_6FF"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTUMYNH3_6FG"
      },
      "outputs": [],
      "source": [
        "# ML Model - 6 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
        "\n",
        "# Initialize the model\n",
        "naive = GaussianNB()\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=4, n_repeats=4, random_state=0)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "GridSearch = GridSearchCV(naive, param_grid, cv=rskf, n_jobs=-1)\n",
        "\n",
        "# Fit the GridSearchCV to the training data\n",
        "GridSearch.fit(x_train, y_train)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = GridSearch.best_params_\n",
        "print(\"Best hyperparameters: \", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate model with best parameters\n",
        "nb_model2 = GaussianNB(var_smoothing = best_params['var_smoothing'])"
      ],
      "metadata": {
        "id": "A0qQ_EXBMKi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "nb2_score = evaluate_model(nb_model2, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "2hSfSTH8Mnb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['Naive Bayes tuned']= nb2_score"
      ],
      "metadata": {
        "id": "Ns2M_He3MtzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYFMyHqX_6FG"
      },
      "source": [
        "##### Which hyperparameter optimization technique have i used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ26Qw2d_6FG"
      },
      "source": [
        "Here we have used the GridSearchCV for optimization of the Naive Bayes model.\n",
        "\n",
        "GridSearchCV is an exhaustive search method that tries all possible combinations of hyperparameters specified in the hyperparameter grid. This technique can be useful when the number of hyperparameters to tune is small and the range of possible values for each hyperparameter is limited. GridSearchCV can find the best combination of hyperparameters, but it can be computationally expensive for large hyperparameter grids."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4hU8F0I_6FG"
      },
      "source": [
        "##### Have i seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Evaluation metric Score Chart\n",
        "score"
      ],
      "metadata": {
        "id": "nZaUzFFkM2AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSpzVU16_6FG"
      },
      "source": [
        "It appears that hyperparameter tuning improved the performance of the Naive Bayes model on the train set. The tuned Naive Bayes model has precision, recall, accuracy and F1 score on the test set as same as in the untuned Naive Bayes model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NNpISRdaSng"
      },
      "source": [
        "### ML Model - 7 : Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmPQ4B8CaSng"
      },
      "outputs": [],
      "source": [
        "# ML Model - 7 Implementation\n",
        "nn_model = MLPClassifier(random_state=0)\n",
        "\n",
        "# Model is trained (fit) and predicted in the evaluate model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IvVWbfDaSnh"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_he51ZUaSnh"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "neural_score = evaluate_model(nn_model, x_train, x_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Evaluation metric Score Chart\n",
        "score['Neural Network'] = neural_score\n",
        "score"
      ],
      "metadata": {
        "id": "tULE2aTOaSnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bSwMgKcaSnh"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffpQHwZaaSni"
      },
      "outputs": [],
      "source": [
        "# ML Model - 7 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {'hidden_layer_sizes': np.arange(10, 100, 10),\n",
        "              'alpha': np.arange(0.0001, 0.01, 0.0001)}\n",
        "\n",
        "# Initialize the model\n",
        "neural = MLPClassifier(random_state=0)\n",
        "\n",
        "# Repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=0)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(neural, param_grid, n_iter=10, cv=rskf, n_jobs=-1)\n",
        "\n",
        "# Fit the RandomizedSearchCV to the training data\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "print(\"Best hyperparameters: \", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate model with best parameters\n",
        "nn_model2 = MLPClassifier(hidden_layer_sizes = best_params['hidden_layer_sizes'],\n",
        "                        alpha = best_params['alpha'],\n",
        "                        random_state = 0)"
      ],
      "metadata": {
        "id": "9MhZA1MeaSni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "neural2_score = evaluate_model(nn_model2, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "GnbfV8jjaSni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['Neural Network tuned']= neural2_score"
      ],
      "metadata": {
        "id": "aek02QwVaSni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXHnAaJfaSni"
      },
      "source": [
        "##### Which hyperparameter optimization technique have i used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZrwfe8GaSni"
      },
      "source": [
        "Here we have used Randomized search to tune the Neural Network model.\n",
        "\n",
        "Randomized search is a popular technique because it can be more efficient than exhaustive search methods like grid search. Instead of trying all possible combinations of hyperparameters, randomized search samples a random subset of the hyperparameter space. This can save time and computational resources while still finding good hyperparameters for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSoGk_2RaSnj"
      },
      "source": [
        "##### Have i seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Evaluation metric Score Chart\n",
        "score"
      ],
      "metadata": {
        "id": "q0aoe5RmaSnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRsBoyiUaSnj"
      },
      "source": [
        "It appears that hyperparameter tuning did not improve the performance of the neural network model on the test set. The precision, recall, accuracy and F1 scores on the test set are the same for both the untuned and tuned neural network models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(score.to_markdown())"
      ],
      "metadata": {
        "id": "U6asxmA7aSnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Selection of best model***"
      ],
      "metadata": {
        "id": "UisOqWiDb6SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the overfitted models which have precision, recall, f1 scores for train as 1\n",
        "score_t = score.transpose()            # taking transpose of the score dataframe to create new difference column\n",
        "remove_models = score_t[score_t['Recall Train']>=0.99].index  # creating a list of models which have 1 for train and score_t['Accuracy Train']==1.0 and score_t['Precision Train']==1.0 and score_t['F1 macro Train']==1.0\n",
        "remove_models\n",
        "\n",
        "adj = score_t.drop(remove_models)                     # creating a new dataframe with required models\n",
        "adj"
      ],
      "metadata": {
        "id": "fjJYcn8HHMRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_best_model(df, metrics):\n",
        "\n",
        "    best_models = {}\n",
        "    for metric in metrics:\n",
        "        max_test = df[metric + ' Test'].max()\n",
        "        best_model_test = df[df[metric + ' Test'] == max_test].index[0]\n",
        "        best_model = best_model_test\n",
        "        best_models[metric] = best_model\n",
        "    return best_models"
      ],
      "metadata": {
        "id": "rma9TJDoilsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = ['Precision', 'Recall', 'Accuracy', 'F1 macro']\n",
        "\n",
        "best_models = select_best_model(adj, metrics)\n",
        "print(\"The best models are:\")\n",
        "for metric, best_model in best_models.items():\n",
        "    print(f\"{metric}: {best_model} - {adj[metric+' Test'][best_model].round(4)}\")"
      ],
      "metadata": {
        "id": "WlQYp9uRc9cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take recall as the primary evaluation metric\n",
        "score_smpl = score.transpose()\n",
        "remove_overfitting_models = score_smpl[score_smpl['Recall Train']>=0.99].index\n",
        "remove_overfitting_models\n",
        "new_score = score_smpl.drop(remove_overfitting_models)\n",
        "new_score = new_score.drop(['Precision Train','Precision Test','Accuracy Train','Accuracy Test','F1 macro Train','F1 macro Test'], axis=1)\n",
        "new_score.index.name = 'Classification Model'\n",
        "print(new_score.to_markdown())"
      ],
      "metadata": {
        "id": "kS_pGstoU6El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did i consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After carefully considering the potential consequences of false positives and false negatives in the context of our business objectives, I have selected recall as the primary evaluation metric for our Iris flower classification model. This means that our goal is to maximize the number of true positives (correctly identified the different iris flowers) while minimizing the number of false negatives (incorrectly identified the flowers not a iris flower). By doing so, we aim to ensure that we correctly identify as many different iris flowers, even if it means that we may have some false positives."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did i choose from the above created models as our final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After evaluating the performance of several machine learning models on the Iris dataset, I have selected the Neural Network as our final prediction model. This decision was based on the model’s performance on our primary evaluation metric of recall, which measures the ability of the model to correctly identify different iris flowers. In our analysis, we found that the Neural Network had the highest recall score among the models we evaluated.\n",
        "\n",
        "I choose recall as the primary evaluation metric because correctly identifying different iris flowers are critical to achieving our business objectives. By selecting a model with a high recall score, we aim to ensure that we correctly identify as many different iris flowers as possible, even if it means that we may have some false positives. Overall, we believe that the Neural Network is the best choice for our needs and will help us achieve a positive business impact."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which i have used for the prediction"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of category labels for reference.\n",
        "Category_RF = ['Iris-Setosa', 'Iris-Versicolor', 'Iris-Virginica']"
      ],
      "metadata": {
        "id": "sCRRWimo6E8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this example, it's a data point with Sepal Length, Sepal Width, Petal Length, and Petal Width.\n",
        "x_nn = np.array([[5.1, 3.5, 1.4, 0.2]])\n",
        "\n",
        "# Use the untrained neural network model (nn_model) to make a prediction.\n",
        "x_nn_prediction = nn_model.predict(x_nn)\n",
        "x_nn_prediction[0]\n",
        "\n",
        "# Display the predicted category label.\n",
        "print(Category_RF[int(x_nn_prediction[0])])"
      ],
      "metadata": {
        "id": "SjxiBmCh7gJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Iris flower classification project, the Neural Network model has been selected as the final prediction model. The project aimed to classify Iris flowers into three distinct species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica. After extensive data exploration, preprocessing, and model evaluation, the following conclusions can be drawn:\n",
        "\n",
        "1. **Data Exploration:** Through a thorough examination of the dataset, we gained insights into the characteristics and distributions of features. We found that Iris-Setosa exhibited distinct features compared to the other two species.\n",
        "\n",
        "2. **Data Preprocessing:** Data preprocessing steps, including handling missing values and encoding categorical variables, were performed to prepare the dataset for modeling.\n",
        "\n",
        "3. **Model Selection:** After experimenting with various machine learning models, Neural Network was chosen as the final model due to its simplicity, interpretability, and good performance in classifying Iris species.\n",
        "\n",
        "4. **Model Training and Evaluation:** The Neural Network model was trained on the training dataset and evaluated using appropriate metrics. The model demonstrated satisfactory accuracy and precision in classifying Iris species.\n",
        "\n",
        "5. **Challenges and Future Work:** The project encountered challenges related to feature engineering and model fine-tuning. Future work may involve exploring more advanced modeling techniques to improve classification accuracy further.\n",
        "\n",
        "6. **Practical Application:** The Iris flower classification model can be applied in real-world scenarios, such as botany and horticulture, to automate the identification of Iris species based on physical characteristics.\n",
        "\n",
        "In conclusion, the Iris flower classification project successfully employed Neural Network as the final prediction model to classify Iris species. The project's outcomes have practical implications in the field of botany and offer valuable insights into feature importance for species differentiation. Further refinements and enhancements may lead to even more accurate and reliable classification models in the future."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    }
  ]
}